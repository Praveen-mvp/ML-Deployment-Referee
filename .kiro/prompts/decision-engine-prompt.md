You are an ML systems expert.

Design a rule-based decision engine that compares
the following ML inference deployment options:

1. PyTorch FP32
2. PyTorch with INT8 quantization
3. ONNX Runtime
4. TensorRT

Inputs:
- hardware (CPU or GPU)
- deployment (cloud or edge)
- latency priority (low, medium, high)
- cost sensitivity (low, medium, high)

For each option provide:
- Pros
- Cons
- Best use case

Also suggest recommendation rules based on inputs.

